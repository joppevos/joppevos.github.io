---
title: "Pipeline to test Dirty-Data"
date: 2019-08-28T10:36:17+02:00
draft: false
---
![diagram.png](/images/post_01/background.jpg)

#### Untested data is dirty
Testing on assumptions you have with tests is the standard in software development for decades. In a new field like data science it is not so common, but should be just as important. 

Problems do not occur in the database, a database can mostly store whatever you give it. The problems are found way further down stream when the predictions are made and information is already displayed to the client. The damage is done and now you get the job from your boss to look into it. You are not sure where it goes wrong, run some queries against the database to find out, write a small report and rerun the transforming and loading for the days where it went wrong. 

This is a situation that I noticed at multiple companies where data is not the product, so not the main focus. 

There are loads of open source frameworks out there to write tests for software integration, but not so much for data checks. In the end a test is a test. All it does is make an assertion if its True or False. The only difference with data tests is that there needs to be database connections, completely cloud based and a way to schedule the interval for the tests.
I decided to modify the framework pytest to my needs and deploy it on AWS.

#### Pipeline diagram
The rough plan was to make it easy for people to write tests and make a dashboard that gives information when the test run and if it failed the person responsible for that client should be notified immediately. Below is a diagram of the complete pipeline.

![diagram.png](/images/post_01/diagram.png)


Each client has an EC2 where a written tests is automatically pulled from git. The tests will make assertions on the RDS database living on the clients AWS account.
---
title: "Pipeline to test Dirty-Data"
date: 2019-08-28T10:36:17+02:00
draft: false
---
![diagram.png](/images/post_01/background.jpg)

## data is dirty
Testing on assumptions you have with tests is the standard in software development for decades. In a new field like data science it is not so common, but should be just as important. 

Problems do not occur in the database, a database can mostly store whatever you give it. The problems are found way further down stream when the predictions are made and information is already displayed to the client. The damage is done and now you get the job from your boss to look into it. You are not sure where it goes wrong, run some queries against the database to find out, write a small report and rerun the transforming and loading for the days where it went wrong. 

This is a situation that I noticed at multiple companies where data is not the product, so not the main focus. 

There are loads of open source frameworks out there to write tests for software integration, but not so much for data checks. In the end a test is a test. All it does is make an assertion if its True or False. The only difference with data tests is that there needs to be database connections, completely cloud based and a way to schedule the interval for the tests.
I decided to modify the framework pytest to my needs and deploy it on AWS.

The rough plan was to make it easy for people to write tests and make a dashboard that gives information when the test run and if it failed the person responsible for that client should be notified immediately. Below is a diagram of the complete pipeline.

![diagram.png](/images/post_01/diagram.png)


Each client has an EC2 where a written tests is automatically pulled from git. The tests will make assertions on the RDS database living on the clients AWS account. The test can be scheduled by pytest markers, a marker is a way to 'mark' a test to group them together. 

```
@pytest.mark.daily
def a_simple_test():
```


#### pytest
I decided on the markers daily, hourly and weekly since that will cover most of it. The status of each tests will be send to an SNS topic. People and services can subscribe to this topic. If something fails SNS will notify . Lambda is also subscribed to the topic and parse the JSON with all information from SNS. Lambda than writes it in a RDS where a simple flask dashboard reads it out.

The dashboard currently displayed on a tv in the office. See image below.
Every client has its own card where the running processes and tests show green or red. 

![diagram.png](/images/post_01/overview_01.png)

When you click on a clients card it will display all the running test on the EC2 with some information about runtime and which marker is used.

![diagram.png](/images/post_01/overview_02.png)

#### Conclusion
Overall it is an interesting project that is growing every day with newly written data tests. It gives confidence when providing solutions that there is no garbage coming into your model.







#### Pytest
Pytest provides the option to decoraty test with markers. Markers are a way to "mark" or group tests.
I decided on the markers daily, hourly and weekly since that will cover most of it. The status of each tests will be send to an SNS topic. People and services can subscribe to this topic. If something fails SNS will notify . Lambda is also subscribed to the topic and parse the JSON with all information from SNS. Lambda than writes it in a RDS where a simple flask dashboard reads it out.

The dashboard currently displayed on a tv in the office. See image below.
Every client has its own card where the running processes and tests show green or red. 

![overview_01.jpg](/images/post_01/overview_01.jpg)

When you click on a clients card it will display all the running test on the EC2 with some information about runtime and which marker is used.

![overview_02.jpg](/images/post_01/overview_02.jpg)

#### Conclusion
Overall it is an interesting project that is growing every day with newly written data tests. It gives confidence when providing solutions that there is no garbage coming into your model.







